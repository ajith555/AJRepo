import requests
from bs4 import BeautifulSoup
import wget

def download_pdfs_with_auth(url, username, password):
    # Create a session to persist the login session
    session = requests.Session()

    # Login using your credentials
    login_payload = {'username': username, 'password': password}
    login_url = 'your_login_url'
    session.post(login_url, data=login_payload)

    # Send a GET request to the protected website
    response = session.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find all links on the page
        links = soup.find_all('a')

        # Iterate through the links
        for link in links:
            href = link.get('href')

            # Check if the link ends with '.pdf'
            if href and href.endswith('.pdf'):
                # Construct the full URL for the PDF document
                pdf_url = url + href if href.startswith('/') else href

                # Download the PDF using wget
                wget.download(pdf_url, out='your_download_directory')

    else:
        print(f"Failed to retrieve the page. Status code: {response.status_code}")

# Replace 'your_website_url', 'your_login_url', 'your_username', and 'your_password'
download_pdfs_with_auth('your_website_url', 'your_username', 'your_password')
